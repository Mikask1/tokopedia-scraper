{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import urllib.parse\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(chromedriver, headless=False):\n",
    "    global driver, session\n",
    "    opt = webdriver.ChromeOptions()\n",
    "    opt.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    if headless:\n",
    "        opt.add_argument(\"--headless\")\n",
    "\n",
    "    return webdriver.Chrome(executable_path=chromedriver, options=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cookies(path):\n",
    "    # For bypassing authentication by using a chrome extension \"Get Cookies.txt LOCALLY\" (does not work on tokopedia)\n",
    "    cookies = []\n",
    "    with open(path) as f:\n",
    "        for index, row in enumerate(f.readlines()):\n",
    "            if index > 3:\n",
    "                cookies.append(\n",
    "                    {\"name\": row.split()[-2], \"value\": row.split()[-1]})\n",
    "    return cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(path):\n",
    "    # File with category column only\n",
    "    clean_cat = []\n",
    "    with open(path, encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            clean_cat.append(list(row.values())[0])\n",
    "    return clean_cat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categories from csv file\n",
    "categories = get_categories(\"resources/category_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download chromedriver.exe with the same version as your Chrome\n",
    "driver = setup(\"resources/chromedriver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(detail_container, category, rank):\n",
    "    # Scrape to get all parameters\n",
    "    \n",
    "    detail = dict()\n",
    "    detail['rank'] = rank\n",
    "    detail['category'] = category\n",
    "    # Name\n",
    "    try:\n",
    "        name = detail_container.find_element(By.XPATH, \".//div[@data-testid='spnSRPProdName']\").get_attribute(\"innerHTML\")\n",
    "        detail['name'] = name\n",
    "    except Exception as e:\n",
    "        ...\n",
    "\n",
    "    # Price\n",
    "    try:\n",
    "        price = detail_container.find_element(By.XPATH, \".//div[@data-testid='spnSRPProdPrice']\").get_attribute(\"innerHTML\")\n",
    "        price = float(re.sub('[^0-9]', '', price))\n",
    "        detail['price'] = price\n",
    "    except Exception as e:\n",
    "        ...\n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        location = detail_container.find_element(By.XPATH, \".//span[@data-testid='spnSRPProdTabShopLoc']\").get_attribute(\"innerHTML\")\n",
    "        detail['location'] = location\n",
    "    except Exception as e:\n",
    "        ...\n",
    "        \n",
    "    # Rating\n",
    "    try:\n",
    "        rating = detail_container.find_element(By.XPATH, \".//*[contains(text(),'Terjual')]\").find_element(By.XPATH, \"preceding-sibling::span[2]\").get_attribute(\"innerHTML\")\n",
    "        rating = float(rating)\n",
    "        detail['rating'] = rating\n",
    "    except Exception as e:\n",
    "        detail['rating'] = 0\n",
    "\n",
    "    # Sold\n",
    "    try:\n",
    "        sold = detail_container.find_element(By.XPATH, \".//span[contains(text(),'Terjual')]\").get_attribute(\"innerHTML\")\n",
    "        if (\"rb\" in sold):\n",
    "            sold = int(re.sub('[^0-9]', '', sold))\n",
    "            sold = sold * 1000\n",
    "        else:\n",
    "            sold = int(re.sub('[^0-9]', '', sold))\n",
    "        detail['sold'] = sold\n",
    "    except Exception as e:\n",
    "        detail['sold'] = 0\n",
    "    \n",
    "    return detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] # Reset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, 11):\n",
    "    print(f\"On page {page}\")\n",
    "    for cat in categories:\n",
    "        url_safe_cat = urllib.parse.quote(cat)\n",
    "        url = f\"https://www.tokopedia.com/search?st=product&q={url_safe_cat}&page={page}\"\n",
    "        print(f'Scraping for category {cat}..')\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        for i in range(2):\n",
    "            time.sleep(1)\n",
    "            containers = driver.find_elements(\n",
    "                By.XPATH, \"//div[@data-testid='master-product-card']\")\n",
    "            for index, container in enumerate(containers):\n",
    "                detail_container = container.find_element(By.TAG_NAME, \"div\").find_element(\n",
    "                    By.TAG_NAME, \"div\").find_elements(By.XPATH, \"./div\")[1].find_element(By.TAG_NAME, \"a\")\n",
    "                data.append(get_details(detail_container,\n",
    "                            cat, (80*(page-1)) + index + 1))\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000);\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39113"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "clean_data = [dict(t) for t in {tuple(d.items()) for d in data} if 'name' in dict(t)]\n",
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dict to csv\n",
    "keys = clean_data[0].keys()\n",
    "\n",
    "with open('resources/item_details.csv', 'w', encoding='utf-8', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(clean_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
